# Turbulent-Modelling-using-Machine-Learning-Techniques
Jie Bao, MASc. Thesis

Recent improvement in GPUs allows for research in fundamental turbulence using Machine
Learning. Current research area concerns the application of a neural network for the canonical
turbulent channel case. Strategy includes building a loss function for the production and
dissipation variables and feature pruning techniques. In turn improving the accuracy and
efficiency of turbulent-viscosity models. The research objective is achieved using Deviate from CAL 
and also Graham from Compute Canada. The high-order DNS data solution is solved using open-source solver PyFR (Discontinuous Galerkin Method).
